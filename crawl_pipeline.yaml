# DearReader AI-Enhanced Crawl Pipeline Configuration
# ============================================================================
#
# DearReader is a focused content processing service that provides:
# ✅ CLI and API interfaces for single URL crawling
# ✅ AI-powered content extraction and processing
# ✅ Configurable processing pipelines
# ✅ Programmatic pipeline steps (database updates, etc.)
#
# DearReader does NOT provide:
# ❌ Queue management or batch processing
# ❌ XML/sitemap parsing
# ❌ URL discovery or crawling multiple URLs
# ❌ Built-in scheduling or automation
#
# Usage Examples:
#   CLI: dearreader crawl "https://example.com/article"
#   API: GET localhost:3000/json/https://example.com/article
#   API: POST localhost:3000/crawl with JSON body {"url": "https://example.com"}
#
# ============================================================================

# ============================================================================
# AI PROVIDER CONFIGURATIONS
# ============================================================================
# Configuration for AI providers and models

ai_providers:
  # OpenRouter providers (cost-effective)
  openrouter-small:
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    model: "microsoft/wizardlm-2-8x22b"
    temperature: 0.2
    rpm_limit: 100
    max_tokens: 4096
    request_timeout_ms: 30000
    max_retries: 2

  openrouter-big:
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    model: "anthropic/claude-3-haiku"
    temperature: 0.2
    rpm_limit: 50
    max_tokens: 8192
    request_timeout_ms: 45000
    max_retries: 3

  # Primary text processing (balanced performance/cost)
  primary_text:
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    model: "anthropic/claude-3-sonnet"
    temperature: 0.1
    rpm_limit: 30
    max_tokens: 16384
    request_timeout_ms: 60000
    max_retries: 3

  # Fast processing (speed optimized)
  fast_processing:
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model: "gpt-3.5-turbo"
    temperature: 0.2
    rpm_limit: 3500
    max_tokens: 4096
    request_timeout_ms: 30000
    max_retries: 2

  # Primary vision processing (for OCR/images)
  primary_vision:
    provider: "gemini"
    api_key: "${GEMINI_API_KEY}"
    base_url: "https://generativelanguage.googleapis.com/v1"
    model: "gemini-pro-vision"
    temperature: 0.1
    rpm_limit: 60
    max_tokens: 8192
    request_timeout_ms: 45000
    max_retries: 3

  # OpenRefine providers (for compatibility)
  openrefine-free:
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    model: "microsoft/wizardlm-2-8x22b"
    temperature: 0.2
    rpm_limit: 18
    max_tokens: 4096
    request_timeout_ms: 30000
    max_retries: 2

  openrefine-premium:
    provider: "openrouter"
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    model: "anthropic/claude-3-haiku"
    temperature: 0.2
    rpm_limit: 180
    max_tokens: 8192
    request_timeout_ms: 45000
    max_retries: 3

  # High-quality processing (when quality matters most)
  openai-gpt-4-turbo:
    provider: "openai"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    model: "gpt-4-turbo-preview"
    temperature: 0.1
    rpm_limit: 500
    max_tokens: 16384
    request_timeout_ms: 60000
    max_retries: 3

# ============================================================================
# RATE LIMITING CONFIGURATION
# ============================================================================

rate_limiting:
  # Enable rate limiting
  enabled: true
  # Log file for tracking API usage per key
  usage_log_path: "./logs/api_usage.log"
  # Reset usage counters daily at midnight
  daily_reset_hour: 0
  # Warning threshold (percentage of limit)
  warning_threshold: 80
  # Strict enforcement (block requests when limit exceeded)
  strict_enforcement: true

  # Provider-specific rate limits
  providers:
    openrefine-free:
      rpm_limit: 18  # 90% of 20
      rpd_limit: 900  # 90% of 1000
    openrefine-premium:
      rpm_limit: 180
      rpd_limit: 9000

# ============================================================================
# PIPELINE ROUTING CONFIGURATION
# ============================================================================

pipeline_routing:
  # Default pipeline for regular URLs (no AI processing)
  default: "html_default"

  # Pipeline-specific URL routing
  routes:
    # AI-enhanced processing routes
    "/test_1/": "html_enhanced"
    "/pdf_pipeline/": "pdf_enhanced"
    "/ai_processing/": "html_enhanced"

  # Pipeline definitions
  pipelines:
  # HTML Content Pipeline (NO LLM - default for regular requests)
  html_default:
    name: "HTML Content Processing Pipeline (No AI)"
    description: "Default pipeline for processing HTML web content without AI processing - broad and unopinionated"
    content_type: "html"
    ai_required: false  # Explicitly disable AI for this pipeline
    stages:
      - name: "universal_crawl"
        type: "crawl"
        description: "Universal web page crawling with broad content detection"
        config:
          format: "html"
          include_metadata: true
          include_links: true
          include_images: false
          timeout_ms: 45000
          # Broad content selectors that work across different site structures
          wait_for_selector: "main, article, .content, .post, .entry, #content, #main, body"
          # Fallback selectors if primary ones don't exist
          fallback_selectors: [
            "[role='main']",
            ".article-content",
            ".post-content",
            ".entry-content",
            ".blog-post",
            ".news-article",
            ".page-content",
            "article",
            ".article",
            ".content-wrapper",
            ".main-content",
            "body"
          ]
          # User agent rotation to avoid bot detection
          user_agent_rotation: true
          user_agents: [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/91.0.864.59"
          ]
          # Accept various content types
          accept_content_types: ["text/html", "text/plain", "application/xhtml+xml"]
          # Error handling and retries
          retry_on_failure: true
          max_retries: 3
          retry_delay_ms: 1000
          retry_backoff_multiplier: 2.0
          timeout_ms: 45000
          # Circuit breaker pattern
          circuit_breaker_enabled: true
          circuit_breaker_failure_threshold: 5
          circuit_breaker_reset_timeout_ms: 60000
          # Caching configuration
          enable_caching: true
          cache_ttl_seconds: 3600  # 1 hour default
          cache_key_strategy: "url+options"  # Options: url_only, url+options, content_hash
          cache_invalidation_strategy: "time_based"  # Options: time_based, manual, smart
          cache_compression: true
          cache_max_size_mb: 100

      - name: "universal_text_extraction"
        type: "text_extract"
        description: "Extract readable text from any web page structure"
        config:
          preserve_formatting: true
          remove_scripts: true
          remove_styles: true
          remove_navigation: true
          remove_ads: true
          remove_comments: true
          # Extract from multiple content areas
          content_selectors: [
            "main",
            "article",
            ".content",
            ".post",
            ".entry",
            "#content",
            "#main",
            ".article-content",
            ".post-content",
            ".entry-content",
            ".blog-post",
            ".news-article",
            ".page-content",
            "[role='main']",
            ".main-content",
            ".content-wrapper"
          ]
          # Preserve important structural elements
          preserve_headers: true
          preserve_lists: true
          preserve_tables: true
          preserve_links: true
          preserve_images: false
          # Clean up common web artifacts
          remove_common_artifacts: true
          # Handle different encodings
          encoding_detection: true
          # Extract metadata from page
          extract_title: true
          extract_description: true
          extract_keywords: true
          extract_author: true
          extract_date: true

      - name: "security_sanitization"
        type: "security_sanitization"
        description: "Sanitize content to prevent prompt injection and security issues"
        config:
          remove_script_tags: true
          remove_event_handlers: true
          sanitize_urls: true
          remove_suspicious_patterns: true
          max_content_length: 100000
          allowed_protocols: ["http", "https"]
          block_patterns: [
            "javascript:",
            "data:",
            "vbscript:",
            "onload=",
            "onerror=",
            "onclick=",
            "<script",
            "</script>"
          ]

      - name: "universal_json_formatting"
        type: "json_format"
        description: "Format result as comprehensive JSON with all extracted data"
        config:
          include_url: true
          include_timestamp: true
          include_request_metadata: true
          include_response_metadata: true
          include_content_metadata: true
          include_extracted_metadata: true
          include_processing_stats: true
          # Structure the output
          output_structure: "comprehensive"
          # Include raw content for debugging
          include_raw_content: false
          # Include extracted text
          include_cleaned_text: true
          # Include detected language
          include_language: true
          # Include content quality score
          include_quality_score: true
          # Include processing warnings/errors
          include_warnings: true    # Enhanced HTML Pipeline (with additional LLM processing)
    html_enhanced:
      name: "Enhanced HTML Processing Pipeline"
      description: "Extended pipeline with categorization and business info extraction"
      content_type: "html"
      extends: "html_default"
      stages:
        - name: "auto_crawl"
          type: "crawl"
          description: "Automatically crawl and extract HTML content"
          config:
            format: "html"
            include_metadata: true
            include_links: true
            include_images: false
            timeout_ms: 30000
            wait_for_selector: "main, article, .content, body"

        - name: "convert_to_markdown"
          type: "llm_process"
          description: "Convert HTML to clean Markdown"
          llm_provider: "fast_processing"
          prompt: "html_to_markdown"

        - name: "categorize_content"
          type: "llm_process"
          description: "Categorize content by type and topic"
          llm_provider: "primary_text"
          prompt: "content_categorization"
          config:
            input_field: "markdown_content"
            output_field: "categorization"

        - name: "extract_business_info"
          type: "llm_process"
          description: "Extract business-related information"
          llm_provider: "primary_text"
          prompt: "business_info_extraction"
          config:
            input_field: "markdown_content"
            output_field: "business_info"
            conditional: "content_contains_business_keywords"

        - name: "extract_metadata"
          type: "llm_process"
          description: "Extract metadata from content"
          llm_provider: "primary_text"
          prompt: "extract_metadata"

        - name: "format_as_json"
          type: "llm_process"
          description: "Format final result as JSON"
          llm_provider: "fast_processing"
          prompt: "json_formatting"

    # PDF Content Pipeline
    pdf_default:
      name: "PDF Content Processing Pipeline"
      description: "Default pipeline for processing PDF documents"
      content_type: "pdf"
      stages:
        - name: "auto_crawl"
          type: "crawl"
          description: "Download PDF file"
          config:
            format: "binary"
            include_metadata: true
            timeout_ms: 60000
            max_file_size_mb: 50

        - name: "extract_text"
          type: "pdf_extract"
          description: "Extract text from PDF using pdf-lib"
          config:
            enable_ocr_fallback: false
            extract_tables: false
            preserve_formatting: true
            max_pages: 100

        - name: "quality_check"
          type: "llm_process"
          description: "Check if extracted content makes sense"
          llm_provider: "primary_text"
          prompt: "pdf_quality_check"
          config:
            input_field: "extracted_text"
            output_field: "quality_assessment"
            quality_threshold: 7.0

        - name: "ocr_extraction"
          type: "llm_process"
          description: "Re-extract with OCR if quality is poor"
          llm_provider: "primary_vision"
          prompt: "pdf_ocr_extraction"
          config:
            input_field: "extracted_text"
            output_field: "ocr_text"
            conditional: "quality_score < 7.0"
            combine_with_original: true

        - name: "format_as_json"
          type: "llm_process"
          description: "Format final result as JSON"
          llm_provider: "fast_processing"
          prompt: "json_formatting"
          config:
            combine_fields: ["final_text", "metadata", "quality_assessment", "processing_method"]

    # PDF Enhanced Pipeline (with additional processing)
    pdf_enhanced:
      name: "Enhanced PDF Processing Pipeline"
      description: "Extended PDF pipeline with advanced OCR and analysis"
      content_type: "pdf"
      extends: "pdf_default"
      stages:
        - name: "auto_crawl"
          type: "crawl"
          description: "Download PDF file"
          config:
            format: "binary"
            include_metadata: true
            timeout_ms: 60000
            max_file_size_mb: 50

        - name: "extract_text"
          type: "pdf_extract"
          description: "Extract text from PDF using pdf-lib"
          config:
            enable_ocr_fallback: true
            extract_tables: true
            preserve_formatting: true
            max_pages: 100

        - name: "quality_check"
          type: "llm_process"
          description: "Check if extracted content makes sense"
          llm_provider: "primary_text"
          prompt: "pdf_quality_check"

        - name: "ocr_extraction"
          type: "llm_process"
          description: "Advanced OCR extraction with vision model"
          llm_provider: "primary_vision"
          prompt: "pdf_ocr_extraction"
          config:
            use_advanced_ocr: true
            handle_multiple_columns: true
            preserve_layout: true

        - name: "content_analysis"
          type: "llm_process"
          description: "Analyze PDF content structure and type"
          llm_provider: "primary_text"
          prompt: "content_categorization"
          config:
            input_field: "final_text"
            output_field: "content_analysis"

        - name: "format_as_json"
          type: "llm_process"
          description: "Format final result as JSON"
          llm_provider: "fast_processing"
          prompt: "json_formatting"

# ============================================================================
# TASK-SPECIFIC AI MODEL ASSIGNMENTS
# ============================================================================
# Map tasks to specific provider-model combinations

ai_tasks:
  # PDF processing tasks - Use local model for speed, cloud for accuracy
  parse_pdf: "openrouter-small" # want fast PDF parsing
  parse_pdf_backup: "openrouter-big" # Backup: Cloud model for complex PDFs

  # Content validation - Use more capable models
  validate_format: "openrouter-small" # want reliable validation
  validate_format_backup: "openrouter-big" # Backup: Cloud model for reliable validation

  # Web crawling and content editing
  edit_crawl: "openrouter-small"
  edit_crawl_backup: "openrouter-big"

  # General purpose tasks - Use cost-effective models
  general_chat: "openrouter-small"
  general_chat_backup: "openrouter-big"

  # Code analysis - Use specialized models
  code_analysis: "openrouter-small"
  code_analysis_backup: "openrouter-big"

  # OCR and document processing
  ocr_processing: "primary_vision" # Use vision model for OCR
  ocr_processing_backup: "openrouter-big"   # Backup: Claude for text analysis

  # Sentiment analysis
  sentiment_analysis: "openrouter-small" # We want nuanced analysis
  sentiment_analysis_backup: "openrouter-big"

  # Content classification
  content_classification: "openrouter-small" # We want nuanced analysis
  content_classification_backup: "openrouter-big" # Backup: More accurate

  # HTML processing tasks
  html_to_markdown: "fast_processing"
  content_categorization: "primary_text"
  business_info_extraction: "primary_text"
  extract_metadata: "openrouter-small"

  # JSON processing tasks
  json_formatting: "fast_processing"
  json_validation: "openrouter-small"

  # Default fallbacks (by priority)
  default: "openrouter-small"
  default_backup: "openrefine-free"
  default_fallback: "fast_processing"

  # High-quality processing (when quality matters most)
  high_quality: "openai-gpt-4-turbo"
  high_quality_backup: "openrouter-big"

  # Fast processing (when speed matters most)
  fast: "fast_processing"
  fast_backup: "openrouter-small"

  # Vision processing (for images/OCR)
  vision: "primary_vision"
  vision_backup: "gemini-pro"

# ============================================================================
# A/B TESTING CONFIGURATION
# ============================================================================
# Configuration for A/B testing different AI models for the same tasks

ab_testing:
  enabled: true
  experiments:
    # Test different models for content categorization
    content_categorization_ab:
      name: "Content Categorization A/B Test"
      description: "Compare different models for content categorization accuracy and speed"
      variants:
        control: "openrouter-small"  # Baseline model
        variant_a: "primary_text"    # Test model A
        variant_b: "openrouter-big"  # Test model B
      traffic_distribution: [50, 25, 25]  # 50% control, 25% A, 25% B
      metrics:
        - accuracy
        - speed
        - cost
      duration_days: 30
      min_samples_per_variant: 1000

    # Test different models for HTML to Markdown conversion
    html_to_markdown_ab:
      name: "HTML to Markdown A/B Test"
      description: "Compare different models for HTML to Markdown conversion quality"
      variants:
        control: "fast_processing"
        variant_a: "openrouter-small"
        variant_b: "primary_text"
      traffic_distribution: [60, 20, 20]
      metrics:
        - quality_score
        - processing_time
        - cost
      duration_days: 14
      min_samples_per_variant: 500

  # Global A/B testing settings
  settings:
    enable_logging: true
    log_sample_rate: 0.1  # Log 10% of requests for analysis
    enable_real_time_metrics: true
    experiment_refresh_interval_hours: 24
    allow_user_opt_out: true
    respect_user_preferences: true

# ============================================================================
# ADVANCED PROMPTING TECHNIQUES
# ============================================================================
# Configuration for advanced prompting strategies

advanced_prompting:
  enabled: true
  techniques:
    # Chain of Thought prompting
    chain_of_thought:
      enabled: true
      tasks: ["content_categorization", "business_info_extraction"]
      template: |
        Let's approach this step by step:

        Step 1: Analyze the content structure and identify key elements
        Step 2: Extract relevant information based on the task requirements
        Step 3: Validate the extracted information for accuracy
        Step 4: Format the final response

        Content: {content}
        Task: {task_description}

        Now, let's work through each step:

    # ReAct (Reasoning + Acting) prompting
    react:
      enabled: true
      tasks: ["extract_metadata", "content_analysis"]
      template: |
        You need to reason about this content and take appropriate actions.

        Observation: {content}
        Task: {task_description}

        Think step by step:
        1. What information do I need to extract?
        2. What patterns should I look for?
        3. How should I validate my findings?
        4. What format should the output be in?

        Action: Analyze the content
        Result: [Your analysis here]

    # Few-shot learning with examples
    few_shot:
      enabled: true
      tasks: ["sentiment_analysis", "content_classification"]
      examples:
        - input: "This product is amazing! I love it!"
          output: '{"sentiment": "positive", "confidence": 0.95}'
        - input: "This is terrible. Complete waste of money."
          output: '{"sentiment": "negative", "confidence": 0.92}'
        - input: "It's okay, nothing special."
          output: '{"sentiment": "neutral", "confidence": 0.78}'

# Prompt Definitions
prompts:
  # HTML Processing Prompts
  html_to_markdown:
    name: "Convert HTML to Markdown"
    description: "Convert HTML content to clean, readable Markdown format"
    version: "1.0.0"
    created_at: "2024-01-15"
    updated_at: "2024-01-15"
    template: |
      Convert the following HTML content to clean, readable Markdown format.
      Preserve all important information, links, and formatting.
      Remove any navigation, ads, or irrelevant content.
      Focus on the main article content.

      HTML Content:
      {content}

      Instructions:
      - Convert headers (h1, h2, etc.) to Markdown headers
      - Convert links to Markdown format [text](url)
      - Preserve code blocks and inline code
      - Keep lists and formatting
      - Remove scripts, styles, and navigation elements
      - Ensure the output is clean and readable

  extract_metadata:
    name: "Extract Content Metadata"
    description: "Extract key metadata from content"
    template: |
      Analyze the following content and extract key metadata.
      Return the information in JSON format.

      Content: {content}

      Extract:
      - title: Main title or headline
      - author: Content author (if available)
      - date: Publication date (if available)
      - description: Brief summary or description
      - keywords: Key topics or tags
      - category: Content category or type
      - language: Content language
      - word_count: Approximate word count

      Return as valid JSON object.

  # PDF Processing Prompts
  pdf_quality_check:
    name: "PDF Content Quality Check"
    description: "Check if extracted PDF content makes sense and is complete"
    template: |
      Analyze the following extracted PDF content and evaluate its quality.
      Check if the content appears complete, readable, and properly formatted.

      PDF Content:
      {content}

      Evaluate:
      1. Is the content readable and properly formatted?
      2. Does the text appear complete (no missing sections)?
      3. Are there any obvious extraction errors or garbled text?
      4. Is the content in the expected language?
      5. Does the content structure make sense?

      Provide a quality score (1-10) and brief explanation.
      If quality is poor (< 7), recommend OCR processing.

  pdf_ocr_extraction:
    name: "PDF OCR Text Extraction"
    description: "Extract text from PDF using OCR capabilities"
    template: |
      You have access to OCR capabilities for processing PDF content.
      The following PDF content appears to have extraction issues.

      Original Content:
      {content}

      Instructions:
      - Use OCR to extract text from the PDF
      - Focus on readable text content
      - Preserve document structure where possible
      - Handle multiple columns or complex layouts
      - Return clean, readable text
      - If OCR fails, return the original content

  # Enhanced HTML Processing Prompts
  content_categorization:
    name: "Content Categorization"
    description: "Categorize content by type and topic"
    template: |
      Analyze the following content and categorize it.

      Content: {content}
      Title: {title}

      Determine:
      1. Content Type: (article, blog, tutorial, news, documentation, etc.)
      2. Main Category: (technology, business, health, education, etc.)
      3. Sub-categories: (specific sub-topics)
      4. Target Audience: (beginners, experts, general public, etc.)
      5. Content Quality: (1-10 score)
      6. Key Topics: (main subjects discussed)

      Return as structured JSON.

  business_info_extraction:
    name: "Business Information Extraction"
    description: "Extract business-related information"
    template: |
      Extract business information from the following content.

      Content: {content}

      Look for and extract:
      - Business Name
      - Address
      - Phone Number
      - Email
      - Website
      - Hours of Operation
      - Services Offered
      - Contact Information
      - Location Details

      Return as JSON object with found information.

  # General Processing Prompts
  json_formatting:
    name: "Format as JSON"
    description: "Format processed content as structured JSON"
    template: |
      Format the following processed content into a structured JSON object.

      Content: {content}
      Metadata: {metadata}
      Additional Info: {additional_info}

      Create a JSON structure with:
      - Basic content information
      - Extracted metadata
      - Processing results
      - Quality metrics
      - Timestamps

      Ensure the JSON is valid and well-structured.

# ============================================================================
# PIPELINE TESTING CONFIGURATION
# ============================================================================
# Test definitions for pipeline stages and features
# These tests validate pipeline functionality without making external requests

pipeline_tests:
  # Test configuration for each pipeline stage type
  stage_tests:
    crawl:
      name: "Crawl Stage Tests"
      description: "Test web crawling functionality"
      tests:
        - name: "basic_html_crawl"
          description: "Test basic HTML page crawling"
          test_url: "http://httpbin.org/html"
          expected_content_type: "text/html"
          expected_selectors: ["h1", "p"]
          timeout_ms: 10000
          validate_response: true

        - name: "json_response_crawl"
          description: "Test JSON response handling"
          test_url: "http://httpbin.org/json"
          expected_content_type: "application/json"
          validate_json: true
          timeout_ms: 5000

        - name: "redirect_handling"
          description: "Test redirect following"
          test_url: "http://httpbin.org/redirect/2"
          expected_final_url: "http://httpbin.org/get"
          validate_redirects: true
          timeout_ms: 15000

    text_extract:
      name: "Text Extraction Tests"
      description: "Test text extraction from HTML"
      tests:
        - name: "basic_text_extraction"
          description: "Extract text from simple HTML"
          test_html: |
            <html><body>
              <h1>Test Title</h1>
              <p>This is a test paragraph with <a href="#test">some links</a>.</p>
              <ul><li>Item 1</li><li>Item 2</li></ul>
            </body></html>
          expected_text_contains: ["Test Title", "test paragraph", "some links"]
          validate_formatting: true

        - name: "complex_html_extraction"
          description: "Extract text from complex HTML with scripts and styles"
          test_html: |
            <html><head><style>body{color:red;}</style></head><body>
              <script>console.log('test');</script>
              <nav>Navigation</nav>
              <article><h1>Main Article</h1><p>Content here</p></article>
              <footer>Footer content</footer>
            </body></html>
          expected_text_contains: ["Main Article", "Content here"]
          expected_text_excludes: ["Navigation", "Footer content", "console.log"]
          validate_cleaning: true

    content_filter:
      name: "Content Filtering Tests"
      description: "Test content filtering and cleaning"
      tests:
        - name: "duplicate_removal"
          description: "Remove duplicate content"
          test_content: "This is duplicate text. This is duplicate text. Unique content here."
          expected_content: "This is duplicate text. Unique content here."
          validate_deduplication: true

        - name: "quality_filtering"
          description: "Filter low-quality content"
          test_content: "Short text"
          min_length: 20
          expected_filtered: true

        - name: "language_detection"
          description: "Detect and filter by language"
          test_content: "This is English content. Ceci est du contenu français."
          expected_language: "en"
          validate_language: true

    json_format:
      name: "JSON Formatting Tests"
      description: "Test JSON output formatting"
      tests:
        - name: "basic_json_formatting"
          description: "Format basic content as JSON"
          test_data:
            title: "Test Title"
            content: "Test content"
            url: "https://example.com"
          expected_fields: ["title", "content", "url", "timestamp"]
          validate_json_structure: true

        - name: "comprehensive_json_formatting"
          description: "Format comprehensive data as JSON"
          test_data:
            title: "Test Article"
            content: "Article content"
            metadata:
              author: "Test Author"
              date: "2024-01-01"
            processing_stats:
              duration_ms: 1500
              stage_count: 3
          expected_nested_fields: ["metadata.author", "processing_stats.duration_ms"]
          validate_comprehensive: true

    pdf_extract:
      name: "PDF Extraction Tests"
      description: "Test PDF text extraction"
      tests:
        - name: "basic_pdf_extraction"
          description: "Extract text from simple PDF"
          test_pdf_content: |
            %PDF-1.4
            1 0 obj
            << /Type /Catalog /Pages 2 0 R >>
            endobj
            2 0 obj
            << /Type /Pages /Kids [3 0 R] /Count 1 >>
            endobj
            3 0 obj
            << /Type /Page /Parent 2 0 R /MediaBox [0 0 612 792] /Contents 4 0 R >>
            endobj
            4 0 obj
            << /Length 68 >>
            stream
            BT
            /F1 12 Tf
            72 720 Td
            (Hello World!) Tj
            ET
            endstream
            endobj
            xref
            0 5
            0000000000 65535 f
            0000000009 00000 n
            0000000058 00000 n
            0000000115 00000 n
            0000000200 00000 n
            trailer
            << /Size 5 /Root 1 0 R >>
            startxref
            284
            %%EOF
          expected_text_contains: ["Hello World"]
          validate_pdf_parsing: true

        - name: "empty_pdf_handling"
          description: "Handle empty or invalid PDFs"
          test_pdf_content: ""
          expected_error: "Failed to extract text from PDF"
          validate_error_handling: true

  # Test configuration for complete pipelines
  pipeline_tests:
    html_default:
      name: "HTML Default Pipeline Tests"
      description: "Test the complete HTML default pipeline"
      test_url: "http://httpbin.org/html"
      expected_output_format: "json"
      expected_fields: ["title", "content", "links", "metadata"]
      validate_no_llm: true
      timeout_ms: 30000

    html_enhanced:
      name: "HTML Enhanced Pipeline Tests"
      description: "Test the complete HTML enhanced pipeline"
      test_url: "http://httpbin.org/html"
      expected_output_format: "json"
      expected_fields: ["title", "content", "categorization", "markdown_content"]
      validate_llm_processing: false  # Set to false to avoid actual LLM calls
      timeout_ms: 45000

    pdf_default:
      name: "PDF Default Pipeline Tests"
      description: "Test the complete PDF default pipeline"
      test_pdf_url: "http://httpbin.org/pdf-sample.pdf"
      expected_output_format: "json"
      expected_fields: ["extracted_text", "metadata", "quality_assessment"]
      validate_pdf_processing: true
      timeout_ms: 60000

  # Test configuration for AI tasks (mocked, no real API calls)
  ai_task_tests:
    parse_pdf:
      name: "PDF Parsing Task Tests"
      description: "Test PDF parsing AI task"
      test_content: "Sample PDF content for testing"
      expected_provider: "openrouter-small"
      validate_task_routing: true
      mock_response: "Parsed PDF content result"

    validate_format:
      name: "Format Validation Task Tests"
      description: "Test format validation AI task"
      test_content: "Content to validate"
      expected_provider: "openrouter-small"
      validate_task_routing: true
      mock_response: "Format validation result"

    content_categorization:
      name: "Content Categorization Task Tests"
      description: "Test content categorization AI task"
      test_content: "Sample article content"
      expected_provider: "primary_text"
      validate_task_routing: true
      mock_response: '{"category": "article", "topics": ["test"]}'

  # Global test settings
  test_settings:
    enable_mock_responses: true
    skip_external_requests: true
    max_test_duration_ms: 120000
    enable_parallel_testing: false
    log_test_results: true
    fail_fast: false
    retry_failed_tests: 2

# ============================================================================
# PERFORMANCE TESTING CONFIGURATION
# ============================================================================
# Configuration for performance and load testing

performance_testing:
  enabled: true

  # Load testing scenarios
  scenarios:
    basic_load_test:
      name: "Basic Load Test"
      description: "Test system performance under normal load"
      duration_minutes: 10
      concurrent_users: 10
      ramp_up_seconds: 60
      requests_per_minute: 100
      target_endpoints:
        - "/json/https://httpbin.org/html"
        - "/json/https://httpbin.org/json"
      expected_response_time_ms: 2000
      expected_success_rate: 0.95

    stress_test:
      name: "Stress Test"
      description: "Test system limits under high load"
      duration_minutes: 5
      concurrent_users: 50
      ramp_up_seconds: 30
      requests_per_minute: 500
      target_endpoints:
        - "/json/https://httpbin.org/html"
        - "/task/html_enhanced/https://httpbin.org/html"
      expected_response_time_ms: 5000
      expected_success_rate: 0.90

    spike_test:
      name: "Spike Test"
      description: "Test system response to sudden traffic spikes"
      duration_minutes: 3
      spike_users: 100
      spike_duration_seconds: 30
      baseline_users: 5
      target_endpoints:
        - "/json/https://httpbin.org/html"
      expected_response_time_ms: 3000
      expected_success_rate: 0.85

  # Performance metrics to track
  metrics:
    response_time:
      enabled: true
      percentiles: [50, 95, 99]
      thresholds:
        p95_max_ms: 3000
        p99_max_ms: 5000

    throughput:
      enabled: true
      measure_rpm: true
      measure_rps: true
      thresholds:
        min_rpm: 50
        max_rpm: 1000

    error_rate:
      enabled: true
      thresholds:
        max_error_rate: 0.05  # 5% max error rate

    resource_usage:
      enabled: true
      track_memory: true
      track_cpu: true
      thresholds:
        max_memory_mb: 1024
        max_cpu_percent: 80

  # End-to-end testing configuration
  e2e_testing:
    enabled: true
    test_scenarios:
      html_processing_e2e:
        name: "HTML Processing End-to-End"
        description: "Complete HTML processing pipeline test"
        steps:
          - name: "crawl_html"
            endpoint: "/json/https://httpbin.org/html"
            expected_status: 200
            validate_response: true
          - name: "validate_content"
            check_fields: ["title", "content", "links"]
          - name: "check_performance"
            max_response_time_ms: 3000

      pdf_processing_e2e:
        name: "PDF Processing End-to-End"
        description: "Complete PDF processing pipeline test"
        steps:
          - name: "crawl_pdf"
            endpoint: "/task/pdf_default/https://httpbin.org/pdf-sample.pdf"
            expected_status: 200
          - name: "validate_pdf_content"
            check_fields: ["extracted_text", "metadata"]
          - name: "check_quality"
            validate_quality_score: true

      ai_enhanced_e2e:
        name: "AI Enhanced Processing End-to-End"
        description: "AI-enhanced pipeline with real API calls"
        steps:
          - name: "enhanced_processing"
            endpoint: "/task/html_enhanced/https://httpbin.org/html"
            expected_status: 200
            skip_without_api_key: true
          - name: "validate_ai_output"
            check_fields: ["categorization", "markdown_content"]
          - name: "performance_check"
            max_response_time_ms: 10000

  # Test data management
  test_data:
    mock_responses: true
    use_cached_responses: true
    generate_real_traffic: false
    data_retention_days: 30
    anonymize_sensitive_data: true

  # Reporting and alerting
  reporting:
    generate_reports: true
    report_format: "json"
    report_frequency: "daily"
    alert_on_failures: true
    alert_thresholds:
      consecutive_failures: 3
      performance_degradation_percent: 20